{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtr6zJTJoqjcJG+/yEwAwj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Gdrive Path mounting"
      ],
      "metadata": {
        "id": "RMGuRM7_8lIA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmHYslVz52Om",
        "outputId": "a2f348dc-3bda-4006-fa75-55cd4f04574c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libs & mod."
      ],
      "metadata": {
        "id": "aAA2bTn29U-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn,optim\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "OUdfbOEc9bU3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the datasets"
      ],
      "metadata": {
        "id": "tzGz9DEM9xws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We need to take the datasets one by one , it reduces the complexity!\n",
        "movies = pd.read_csv('./AutoEncoders/ml-1m/movies.dat', header = None , sep = '::' , engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('./AutoEncoders/ml-1m/ratings.dat', header = None , sep = '::' , engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('./AutoEncoders/ml-1m/users.dat', header = None , sep = '::' , engine = 'python', encoding = 'latin-1')"
      ],
      "metadata": {
        "id": "blvXDI1U9mkh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Preparing the training set and the test set"
      ],
      "metadata": {
        "id": "9s5R3Wi5-mnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = pd.read_csv('./AutoEncoders/ml-100k/u1.base', delimiter= '\\t')\n",
        "test_set = pd.read_csv('./AutoEncoders/ml-100k/u1.test', delimiter= '\\t')"
      ],
      "metadata": {
        "id": "tM8F-cUP-QmK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing"
      ],
      "metadata": {
        "id": "LQJEuZmt_dRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the number of users and movies"
      ],
      "metadata": {
        "id": "ibbMfDti-_uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = np.array(training_set , dtype = 'int')\n",
        "test_set = np.array(test_set , dtype = 'int')"
      ],
      "metadata": {
        "id": "J7Ho_997-8Sb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_users  = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
        "nb_movies  = int(max(max(training_set[:, 1]), max(test_set[:, 1])))"
      ],
      "metadata": {
        "id": "c3Lre9mb_akl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the data into an array with users in lines and movies in columns"
      ],
      "metadata": {
        "id": "UrqmX9u-_wfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(data):\n",
        "  new_data = []\n",
        "  for id_user in range(1 , nb_users + 1):\n",
        "    #We specify that we just want the movies and ratigns of users(id_user)\n",
        "    id_movies = data[:, 1][data[:, 0] == id_user]\n",
        "    id_ratings = data[:, 2][data[:, 0] == id_user]\n",
        "    ratings = np.zeros(nb_movies)\n",
        "    ratings[id_movies - 1] = id_ratings\n",
        "    #We specify that this is going to be a appended as a list\n",
        "    new_data.append(list(ratings))\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "UaL2hY5J_Lqq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = convert(training_set)\n",
        "test = convert(test_set)"
      ],
      "metadata": {
        "id": "D5fJVq2bAgT0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the data into Torch tensors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X6oLHo4hAk9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = torch.FloatTensor(train)\n",
        "test = torch.FloatTensor(test)"
      ],
      "metadata": {
        "id": "K6uOFXH4AjzD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "Wr2LPQKHA1XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        #We are taking the number  of movies as an input for the network and it'll predict the same movies as an output\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "ON6QrKoIAyFJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the SAE"
      ],
      "metadata": {
        "id": "lkeUh6ZTCKc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sae = SAE()\n",
        "n_epochs = 200\n",
        "lr = 1e-2\n",
        "crition = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters() , lr = lr , weight_decay = 0.5)\n",
        "\n",
        "##Training Loop\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train_loss = 0\n",
        "  s = 0.\n",
        "\n",
        "  for id_user in range(nb_users):\n",
        "    input = Variable(train[id_user]).unsqueeze(0)\n",
        "    target = input.clone()\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "      output = sae(input)\n",
        "      target.requires_grad = False\n",
        "      output[target == 0] = 0\n",
        "      loss = crition(output,target)\n",
        "      mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10) ##We want to follow the min max rule\n",
        "      loss.backward()\n",
        "      train_loss += np.sqrt(loss.data * mean_corrector)\n",
        "      s += 1.\n",
        "      optimizer.step()\n",
        "  print('Epoch : ' + str(epoch) + '|| Train Loss : ' + str(train_loss/s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAXk0wRECGcI",
        "outputId": "7a530d9e-5b83-47b7-d95d-2aa44a152167"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1|| Train Loss : tensor(1.7714)\n",
            "Epoch : 2|| Train Loss : tensor(1.0966)\n",
            "Epoch : 3|| Train Loss : tensor(1.0536)\n",
            "Epoch : 4|| Train Loss : tensor(1.0384)\n",
            "Epoch : 5|| Train Loss : tensor(1.0309)\n",
            "Epoch : 6|| Train Loss : tensor(1.0266)\n",
            "Epoch : 7|| Train Loss : tensor(1.0239)\n",
            "Epoch : 8|| Train Loss : tensor(1.0218)\n",
            "Epoch : 9|| Train Loss : tensor(1.0211)\n",
            "Epoch : 10|| Train Loss : tensor(1.0197)\n",
            "Epoch : 11|| Train Loss : tensor(1.0188)\n",
            "Epoch : 12|| Train Loss : tensor(1.0185)\n",
            "Epoch : 13|| Train Loss : tensor(1.0178)\n",
            "Epoch : 14|| Train Loss : tensor(1.0176)\n",
            "Epoch : 15|| Train Loss : tensor(1.0171)\n",
            "Epoch : 16|| Train Loss : tensor(1.0170)\n",
            "Epoch : 17|| Train Loss : tensor(1.0165)\n",
            "Epoch : 18|| Train Loss : tensor(1.0166)\n",
            "Epoch : 19|| Train Loss : tensor(1.0165)\n",
            "Epoch : 20|| Train Loss : tensor(1.0161)\n",
            "Epoch : 21|| Train Loss : tensor(1.0161)\n",
            "Epoch : 22|| Train Loss : tensor(1.0159)\n",
            "Epoch : 23|| Train Loss : tensor(1.0158)\n",
            "Epoch : 24|| Train Loss : tensor(1.0157)\n",
            "Epoch : 25|| Train Loss : tensor(1.0157)\n",
            "Epoch : 26|| Train Loss : tensor(1.0155)\n",
            "Epoch : 27|| Train Loss : tensor(1.0154)\n",
            "Epoch : 28|| Train Loss : tensor(1.0150)\n",
            "Epoch : 29|| Train Loss : tensor(1.0124)\n",
            "Epoch : 30|| Train Loss : tensor(1.0116)\n",
            "Epoch : 31|| Train Loss : tensor(1.0095)\n",
            "Epoch : 32|| Train Loss : tensor(1.0102)\n",
            "Epoch : 33|| Train Loss : tensor(1.0060)\n",
            "Epoch : 34|| Train Loss : tensor(1.0063)\n",
            "Epoch : 35|| Train Loss : tensor(1.0027)\n",
            "Epoch : 36|| Train Loss : tensor(1.0017)\n",
            "Epoch : 37|| Train Loss : tensor(0.9986)\n",
            "Epoch : 38|| Train Loss : tensor(0.9970)\n",
            "Epoch : 39|| Train Loss : tensor(0.9947)\n",
            "Epoch : 40|| Train Loss : tensor(0.9962)\n",
            "Epoch : 41|| Train Loss : tensor(0.9919)\n",
            "Epoch : 42|| Train Loss : tensor(0.9925)\n",
            "Epoch : 43|| Train Loss : tensor(0.9881)\n",
            "Epoch : 44|| Train Loss : tensor(0.9904)\n",
            "Epoch : 45|| Train Loss : tensor(0.9865)\n",
            "Epoch : 46|| Train Loss : tensor(0.9828)\n",
            "Epoch : 47|| Train Loss : tensor(0.9826)\n",
            "Epoch : 48|| Train Loss : tensor(0.9822)\n",
            "Epoch : 49|| Train Loss : tensor(0.9772)\n",
            "Epoch : 50|| Train Loss : tensor(0.9776)\n",
            "Epoch : 51|| Train Loss : tensor(0.9721)\n",
            "Epoch : 52|| Train Loss : tensor(0.9730)\n",
            "Epoch : 53|| Train Loss : tensor(0.9690)\n",
            "Epoch : 54|| Train Loss : tensor(0.9712)\n",
            "Epoch : 55|| Train Loss : tensor(0.9723)\n",
            "Epoch : 56|| Train Loss : tensor(0.9761)\n",
            "Epoch : 57|| Train Loss : tensor(0.9735)\n",
            "Epoch : 58|| Train Loss : tensor(0.9733)\n",
            "Epoch : 59|| Train Loss : tensor(0.9661)\n",
            "Epoch : 60|| Train Loss : tensor(0.9679)\n",
            "Epoch : 61|| Train Loss : tensor(0.9666)\n",
            "Epoch : 62|| Train Loss : tensor(0.9687)\n",
            "Epoch : 63|| Train Loss : tensor(0.9636)\n",
            "Epoch : 64|| Train Loss : tensor(0.9651)\n",
            "Epoch : 65|| Train Loss : tensor(0.9640)\n",
            "Epoch : 66|| Train Loss : tensor(0.9630)\n",
            "Epoch : 67|| Train Loss : tensor(0.9621)\n",
            "Epoch : 68|| Train Loss : tensor(0.9647)\n",
            "Epoch : 69|| Train Loss : tensor(0.9575)\n",
            "Epoch : 70|| Train Loss : tensor(0.9553)\n",
            "Epoch : 71|| Train Loss : tensor(0.9549)\n",
            "Epoch : 72|| Train Loss : tensor(0.9542)\n",
            "Epoch : 73|| Train Loss : tensor(0.9529)\n",
            "Epoch : 74|| Train Loss : tensor(0.9582)\n",
            "Epoch : 75|| Train Loss : tensor(0.9524)\n",
            "Epoch : 76|| Train Loss : tensor(0.9547)\n",
            "Epoch : 77|| Train Loss : tensor(0.9489)\n",
            "Epoch : 78|| Train Loss : tensor(0.9489)\n",
            "Epoch : 79|| Train Loss : tensor(0.9501)\n",
            "Epoch : 80|| Train Loss : tensor(0.9485)\n",
            "Epoch : 81|| Train Loss : tensor(0.9446)\n",
            "Epoch : 82|| Train Loss : tensor(0.9448)\n",
            "Epoch : 83|| Train Loss : tensor(0.9442)\n",
            "Epoch : 84|| Train Loss : tensor(0.9476)\n",
            "Epoch : 85|| Train Loss : tensor(0.9429)\n",
            "Epoch : 86|| Train Loss : tensor(0.9468)\n",
            "Epoch : 87|| Train Loss : tensor(0.9414)\n",
            "Epoch : 88|| Train Loss : tensor(0.9430)\n",
            "Epoch : 89|| Train Loss : tensor(0.9407)\n",
            "Epoch : 90|| Train Loss : tensor(0.9420)\n",
            "Epoch : 91|| Train Loss : tensor(0.9393)\n",
            "Epoch : 92|| Train Loss : tensor(0.9411)\n",
            "Epoch : 93|| Train Loss : tensor(0.9380)\n",
            "Epoch : 94|| Train Loss : tensor(0.9384)\n",
            "Epoch : 95|| Train Loss : tensor(0.9361)\n",
            "Epoch : 96|| Train Loss : tensor(0.9385)\n",
            "Epoch : 97|| Train Loss : tensor(0.9348)\n",
            "Epoch : 98|| Train Loss : tensor(0.9360)\n",
            "Epoch : 99|| Train Loss : tensor(0.9349)\n",
            "Epoch : 100|| Train Loss : tensor(0.9363)\n",
            "Epoch : 101|| Train Loss : tensor(0.9350)\n",
            "Epoch : 102|| Train Loss : tensor(0.9355)\n",
            "Epoch : 103|| Train Loss : tensor(0.9335)\n",
            "Epoch : 104|| Train Loss : tensor(0.9373)\n",
            "Epoch : 105|| Train Loss : tensor(0.9372)\n",
            "Epoch : 106|| Train Loss : tensor(0.9347)\n",
            "Epoch : 107|| Train Loss : tensor(0.9325)\n",
            "Epoch : 108|| Train Loss : tensor(0.9332)\n",
            "Epoch : 109|| Train Loss : tensor(0.9321)\n",
            "Epoch : 110|| Train Loss : tensor(0.9323)\n",
            "Epoch : 111|| Train Loss : tensor(0.9310)\n",
            "Epoch : 112|| Train Loss : tensor(0.9328)\n",
            "Epoch : 113|| Train Loss : tensor(0.9305)\n",
            "Epoch : 114|| Train Loss : tensor(0.9331)\n",
            "Epoch : 115|| Train Loss : tensor(0.9313)\n",
            "Epoch : 116|| Train Loss : tensor(0.9310)\n",
            "Epoch : 117|| Train Loss : tensor(0.9307)\n",
            "Epoch : 118|| Train Loss : tensor(0.9305)\n",
            "Epoch : 119|| Train Loss : tensor(0.9296)\n",
            "Epoch : 120|| Train Loss : tensor(0.9291)\n",
            "Epoch : 121|| Train Loss : tensor(0.9288)\n",
            "Epoch : 122|| Train Loss : tensor(0.9296)\n",
            "Epoch : 123|| Train Loss : tensor(0.9288)\n",
            "Epoch : 124|| Train Loss : tensor(0.9286)\n",
            "Epoch : 125|| Train Loss : tensor(0.9280)\n",
            "Epoch : 126|| Train Loss : tensor(0.9287)\n",
            "Epoch : 127|| Train Loss : tensor(0.9279)\n",
            "Epoch : 128|| Train Loss : tensor(0.9279)\n",
            "Epoch : 129|| Train Loss : tensor(0.9267)\n",
            "Epoch : 130|| Train Loss : tensor(0.9272)\n",
            "Epoch : 131|| Train Loss : tensor(0.9262)\n",
            "Epoch : 132|| Train Loss : tensor(0.9270)\n",
            "Epoch : 133|| Train Loss : tensor(0.9259)\n",
            "Epoch : 134|| Train Loss : tensor(0.9263)\n",
            "Epoch : 135|| Train Loss : tensor(0.9253)\n",
            "Epoch : 136|| Train Loss : tensor(0.9256)\n",
            "Epoch : 137|| Train Loss : tensor(0.9245)\n",
            "Epoch : 138|| Train Loss : tensor(0.9250)\n",
            "Epoch : 139|| Train Loss : tensor(0.9240)\n",
            "Epoch : 140|| Train Loss : tensor(0.9243)\n",
            "Epoch : 141|| Train Loss : tensor(0.9234)\n",
            "Epoch : 142|| Train Loss : tensor(0.9243)\n",
            "Epoch : 143|| Train Loss : tensor(0.9231)\n",
            "Epoch : 144|| Train Loss : tensor(0.9236)\n",
            "Epoch : 145|| Train Loss : tensor(0.9227)\n",
            "Epoch : 146|| Train Loss : tensor(0.9230)\n",
            "Epoch : 147|| Train Loss : tensor(0.9219)\n",
            "Epoch : 148|| Train Loss : tensor(0.9221)\n",
            "Epoch : 149|| Train Loss : tensor(0.9214)\n",
            "Epoch : 150|| Train Loss : tensor(0.9217)\n",
            "Epoch : 151|| Train Loss : tensor(0.9212)\n",
            "Epoch : 152|| Train Loss : tensor(0.9217)\n",
            "Epoch : 153|| Train Loss : tensor(0.9209)\n",
            "Epoch : 154|| Train Loss : tensor(0.9207)\n",
            "Epoch : 155|| Train Loss : tensor(0.9198)\n",
            "Epoch : 156|| Train Loss : tensor(0.9205)\n",
            "Epoch : 157|| Train Loss : tensor(0.9200)\n",
            "Epoch : 158|| Train Loss : tensor(0.9205)\n",
            "Epoch : 159|| Train Loss : tensor(0.9192)\n",
            "Epoch : 160|| Train Loss : tensor(0.9196)\n",
            "Epoch : 161|| Train Loss : tensor(0.9189)\n",
            "Epoch : 162|| Train Loss : tensor(0.9193)\n",
            "Epoch : 163|| Train Loss : tensor(0.9186)\n",
            "Epoch : 164|| Train Loss : tensor(0.9193)\n",
            "Epoch : 165|| Train Loss : tensor(0.9186)\n",
            "Epoch : 166|| Train Loss : tensor(0.9191)\n",
            "Epoch : 167|| Train Loss : tensor(0.9182)\n",
            "Epoch : 168|| Train Loss : tensor(0.9182)\n",
            "Epoch : 169|| Train Loss : tensor(0.9179)\n",
            "Epoch : 170|| Train Loss : tensor(0.9179)\n",
            "Epoch : 171|| Train Loss : tensor(0.9175)\n",
            "Epoch : 172|| Train Loss : tensor(0.9177)\n",
            "Epoch : 173|| Train Loss : tensor(0.9174)\n",
            "Epoch : 174|| Train Loss : tensor(0.9177)\n",
            "Epoch : 175|| Train Loss : tensor(0.9170)\n",
            "Epoch : 176|| Train Loss : tensor(0.9174)\n",
            "Epoch : 177|| Train Loss : tensor(0.9165)\n",
            "Epoch : 178|| Train Loss : tensor(0.9177)\n",
            "Epoch : 179|| Train Loss : tensor(0.9168)\n",
            "Epoch : 180|| Train Loss : tensor(0.9172)\n",
            "Epoch : 181|| Train Loss : tensor(0.9164)\n",
            "Epoch : 182|| Train Loss : tensor(0.9168)\n",
            "Epoch : 183|| Train Loss : tensor(0.9160)\n",
            "Epoch : 184|| Train Loss : tensor(0.9168)\n",
            "Epoch : 185|| Train Loss : tensor(0.9157)\n",
            "Epoch : 186|| Train Loss : tensor(0.9159)\n",
            "Epoch : 187|| Train Loss : tensor(0.9160)\n",
            "Epoch : 188|| Train Loss : tensor(0.9161)\n",
            "Epoch : 189|| Train Loss : tensor(0.9156)\n",
            "Epoch : 190|| Train Loss : tensor(0.9159)\n",
            "Epoch : 191|| Train Loss : tensor(0.9153)\n",
            "Epoch : 192|| Train Loss : tensor(0.9157)\n",
            "Epoch : 193|| Train Loss : tensor(0.9151)\n",
            "Epoch : 194|| Train Loss : tensor(0.9157)\n",
            "Epoch : 195|| Train Loss : tensor(0.9152)\n",
            "Epoch : 196|| Train Loss : tensor(0.9148)\n",
            "Epoch : 197|| Train Loss : tensor(0.9143)\n",
            "Epoch : 198|| Train Loss : tensor(0.9147)\n",
            "Epoch : 199|| Train Loss : tensor(0.9144)\n",
            "Epoch : 200|| Train Loss : tensor(0.9147)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the *SAE*"
      ],
      "metadata": {
        "id": "PVyDJfH7Dxse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "\n",
        "for id_user in range(nb_users):\n",
        "  input = Variable(train[id_user]).unsqueeze(0)\n",
        "  target = Variable(test[id_user]).unsqueeze(0)\n",
        "  if torch.sum(target.data > 0)> 0:\n",
        "    output = sae(input)\n",
        "    target.required_grad = False\n",
        "    output[target == 0] = 0\n",
        "    loss = crition(output,target)\n",
        "    mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
        "    test_loss += np.sqrt(loss.data * mean_corrector)\n",
        "    s += 1.\n",
        "\n",
        "print('Test Loss : ' + str(test_loss/s))\n",
        "\n",
        "\n",
        "##SDC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnWvAR81Didg",
        "outputId": "84f091e7-a32a-42b7-8700-a9540b20eac5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss : tensor(0.9561)\n"
          ]
        }
      ]
    }
  ]
}